Embedding Inference Server

This repository provides a production-ready inference server for running text embedding models (and related tasks) with:
	â€¢	Batching & concurrency control â€“ efficient GPU/CPU utilization
	â€¢	Backpressure & error handling â€“ returns errors when overloaded, instead of hanging
	â€¢	Rate limiting & authentication â€“ per-user / per-key governor
	â€¢	Metrics & observability â€“ structured tracing + Prometheus counters
	â€¢	OpenAI-compatible embedding API â€“ /embed endpoint

â¸»

âœ¨ Features
	â€¢	Flexible Model Loading
	â€¢	Load Hugging Face models (--model-id BAAI/bge-large-en-v1.5) or local directories
	â€¢	Configurable revision, dtype (float16, etc.), and pooling strategy
	â€¢	Embedding API (/embed)
	â€¢	Supports single and batch requests
	â€¢	Configurable truncation, normalization, dimensions, and prompts
	â€¢	Batch-size validation (max_client_batch_size)
	â€¢	Robust Error Handling
	â€¢	Queue full â†’ returns 429 Too Many Requests
	â€¢	Tokenization errors, empty batches, invalid batch sizes â†’ descriptive error JSON
	â€¢	Inference failures â†’ 500 Internal Server Error
	â€¢	Scalable Concurrency Model
	â€¢	Queue + batching task + backend task architecture
	â€¢	max_batch_tokens and max_batch_requests to control GPU/CPU load
	â€¢	max_concurrent_requests for backpressure handling
	â€¢	Rate Limiting & Security
	â€¢	Configurable api_key support
	â€¢	Request governor (80 req/s, burst=50) with background cleanup
	â€¢	Auth middleware (Bearer <API_KEY>)
	â€¢	Observability
	â€¢	JSON or human-readable logs
	â€¢	Tracing spans with optional disabling
	â€¢	Prometheus counters + histograms for:
	â€¢	Request counts/success/failures
	â€¢	Tokenization, queue, and inference timings
	â€¢	Queue size, batch size, and batch token usage

â¸»

ðŸš€ Quickstart

Run Server

cargo run --release -- \
  --model-id BAAI/bge-large-en-v1.5 \
  --max-concurrent-requests 4 \
  --max-batch-tokens 1384 \
  --max-batch-requests 5 \
  --port 8080

The server will start at:

http://0.0.0.0:8080


â¸»

Embed API

Single Input

curl -X POST http://localhost:8080/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": "Hello world",
    "normalize": true
  }'

Batch Input

curl -X POST http://localhost:8080/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": ["First document", "Second document"],
    "truncate": true
  }'


â¸»

ðŸ“¦ Configuration

Flag	Env	Default	Description
--model-id	MODEL_ID	./Qwen3-Embedding-0.6B	HF model ID or local path
--revision	REVISION	none	Hub revision/commit/branch
--tokenization-workers	TOKENIZATION_WORKERS	CPU cores	Parallel tokenizers
--dtype	DTYPE	float16	Force model dtype
--pooling	POOLING	model config	Override pooling
--max-concurrent-requests	MAX_CONCURRENT_REQUESTS	1	Limit concurrent requests
--max-batch-tokens	MAX_BATCH_TOKENS	1384	Max tokens per batch
--max-batch-requests	MAX_BATCH_REQUESTS	5	Max requests per batch
--max-client-batch-size	MAX_CLIENT_BATCH_SIZE	2	Max inputs per client request
--hostname	HOSTNAME	0.0.0.0	Bind address
--port	PORT	8080	HTTP port
--api-key	API_KEY	none	Require bearer token
--json-output	JSON_OUTPUT	false	JSON logs for telemetry
--otlp-endpoint	OTLP_ENDPOINT	none	OpenTelemetry OTLP gRPC endpoint
--otlp-service-name	OTLP_SERVICE_NAME	s3-embedding.server	OTLP service name


â¸»

ðŸ›  Architecture

Client --> /embed handler --> Queue --> Batching Task --> Backend Task --> Model
                |                                             |
                <--------------- response_tx ----------------->

	â€¢	Handler: Parses request, validates batch size, acquires concurrency permit
	â€¢	Queue: Buffers incoming requests with backpressure
	â€¢	Batching Task: Groups requests by token/request limits, dispatches to backend
	â€¢	Backend Task: Runs inference and responds via response_tx
	â€¢	Middleware: API key auth, rate limiting, response wrapping

â¸»

ðŸ“ˆ Example Response

{
  "body": [
    [0.0134, -0.0523, 0.0871, ...]
  ]
}

Error (queue full):

{
  "error": "Queue is full",
  "error_type": "overloaded"
}


â¸»
